# B√°o C√°o ƒê√°nh Gi√° v√† Khuy·∫øn Ngh·ªã C·∫£i Thi·ªán H·ªá Th·ªëng RAG

## üìä T·ªïng Quan ƒê√°nh Gi√°

D·ª±a tr√™n k·∫øt qu·∫£ test trong `testing-query-chatRAG.txt` v√† ph√¢n t√≠ch t·ª´ `r1.md`, h·ªá th·ªëng ƒë√£ ho·∫°t ƒë·ªông c∆° b·∫£n nh∆∞ng c√≤n m·ªôt s·ªë v·∫•n ƒë·ªÅ c·∫ßn c·∫£i thi·ªán v·ªÅ ƒë·ªô ch√≠nh x√°c v√† relevance.

### D·ªØ Li·ªáu Hi·ªán T·∫°i

- **7 t√†i li·ªáu PDF** v·ªõi 2 ch·ªß ƒë·ªÅ ch√≠nh:
  - **M·∫≠t m√£ h·ªçc** (ti·∫øng Vi·ªát): 4 ch∆∞∆°ng v·ªÅ m√£ h√≥a, ch·ªØ k√Ω s·ªë
  - **DBMS/IoT** (ti·∫øng Anh/Vi·ªát): Qu·∫£n tr·ªã c∆° s·ªü d·ªØ li·ªáu, IoT
- **Ng√¥n ng·ªØ h·ªón h·ª£p**: Ti·∫øng Vi·ªát, Anh, v√† truy v·∫•n song ng·ªØ

## üîç Ph√¢n T√≠ch V·∫•n ƒê·ªÅ Hi·ªán T·∫°i

### 1. **V·∫•n ƒê·ªÅ V·ªÅ ƒê·ªô Ch√≠nh X√°c Ngu·ªìn**

```
‚ùå Query: "M√£ h√≥a kh√≥a b·∫•t ƒë·ªëi x·ª©ng"
‚ùå K·∫øt qu·∫£: Tr·∫£ v·ªÅ c·∫£ Ch∆∞∆°ng 2 (ƒë·ªëi x·ª©ng) + kb_dbms (kh√¥ng li√™n quan)
‚úÖ Mong mu·ªën: Ch·ªâ Ch∆∞∆°ng 3 (b·∫•t ƒë·ªëi x·ª©ng) + c√°c thu·∫≠t to√°n PKC
```

**Nguy√™n nh√¢n:**

- Hybrid search thi·∫øu b∆∞·ªõc l·ªçc theo ch·ªß ƒë·ªÅ
- Kh√¥ng c√≥ chu·∫©n h√≥a thang ƒëi·ªÉm vector vs keyword
- Thi·∫øu diversification theo document

### 2. **V·∫•n ƒê·ªÅ V·ªõi Ti·∫øng Vi·ªát**

```
‚ùå Keyword search: "m√£ h√≥a" kh√¥ng match "m√£ h√≥a" (encoding kh√°c nhau)
‚ùå Chunk b·ªã c·∫Øt: Ti√™u ƒë·ªÅ slide + n·ªôi dung t√°ch bi·ªát
‚úÖ C·∫ßn: Chu·∫©n h√≥a kh√¥ng d·∫•u + chunking th√¥ng minh
```

### 3. **V·∫•n ƒê·ªÅ V·ªÅ Thu·∫≠t To√°n**

```python
# Hi·ªán t·∫°i trong hybrid_search()
vec_hits = vector_search(query, k=40)
kw_hits = keyword_search(query, k=20) 
# Ch·ªâ merge theo ID, kh√¥ng l·ªçc ch·∫•t l∆∞·ª£ng
```

**Thi·∫øu:**

- Post-filtering theo relevance threshold
- MMR (Maximal Marginal Relevance) ƒë·ªÉ gi·∫£m tr√πng l·∫∑p
- Topic gating cho low-score results

## üéØ Khuy·∫øn Ngh·ªã C·∫£i Thi·ªán

### **C·∫•p ƒê·ªô 1: C·∫£i Thi·ªán Ngay (Quick Wins)**

#### 1.1 Th√™m Chu·∫©n H√≥a Ti·∫øng Vi·ªát

```sql
-- Th√™m v√†o database
ALTER TABLE chunks ADD COLUMN content_normalized TEXT 
GENERATED ALWAYS AS (unaccent(lower(content))) STORED;

CREATE INDEX idx_chunks_content_norm_gin 
ON chunks USING GIN (content_normalized gin_trgm_ops);
```

#### 1.2 C·∫£i Thi·ªán Keyword Search

```python
# Trong retrieval.py - _keyword_candidates()
def normalize_vietnamese(text):
    import unicodedata
    return unicodedata.normalize('NFD', text.lower()).encode('ascii', 'ignore').decode('ascii')

# S·ª≠a SQL query
sql_trgm = """
    SELECT id, document_id, chunk_index, content, 
           similarity(content_normalized, %s) AS score
    FROM chunks
    WHERE content_normalized ILIKE %s
    ORDER BY score DESC
    LIMIT %s
"""
normalized_query = normalize_vietnamese(query)
rows = conn.execute(sql_trgm, (normalized_query, f'%{normalized_query}%', limit))
```

#### 1.3 TƒÉng Tham S·ªë T√¨m Ki·∫øm

```python
# Trong retrieval.py - hybrid_search()
def hybrid_search(query: str, k: int = 8, k_vec: int = 60, k_kw: int = 30):
    # TƒÉng t·ª´ 40/20 l√™n 60/30 cho ti·∫øng Vi·ªát
```

### **C·∫•p ƒê·ªô 2: Post-Processing Th√¥ng Minh**

#### 2.1 Post-Filter Sau Reranking

```python
def post_filter_results(reranked_results, query, low_score_threshold=0.35, max_per_doc=2):
    """
    L·ªçc k·∫øt qu·∫£ sau reranking ƒë·ªÉ lo·∫°i b·ªè nhi·ªÖu v√† ƒë·∫£m b·∫£o ƒëa d·∫°ng
    """
    # T·ª´ ƒëi·ªÉn ch·ªß ƒë·ªÅ ƒë·ªÉ topic gating
    topic_keywords = {
        "m√£ h√≥a b·∫•t ƒë·ªëi x·ª©ng": ["kh√≥a c√¥ng khai", "PKC", "RSA", "ElGamal", "DSA", "asymmetric"],
        "m√£ h√≥a ƒë·ªëi x·ª©ng": ["AES", "DES", "symmetric", "kh√≥a b√≠ m·∫≠t"],
        "transaction": ["ACID", "commit", "rollback", "giao d·ªãch"],
        "client-server": ["client", "server", "ki·∫øn tr√∫c", "architecture"]
    }
    
    # Chu·∫©n h√≥a query
    query_lower = normalize_vietnamese(query)
    query_terms = set(query_lower.split())
    
    # T√¨m topic keywords ph√π h·ª£p
    relevant_keywords = set()
    for topic, keywords in topic_keywords.items():
        if any(term in query_lower for term in topic.split()):
            relevant_keywords.update([normalize_vietnamese(kw) for kw in keywords])
    
    filtered_results = []
    doc_count = {}
    
    for result in reranked_results:
        score = result.get('relevance_score', 0)
        text_normalized = normalize_vietnamese(result['text'])
        doc_id = result['metadata']['document_id']
        
        # Topic gating cho ƒëi·ªÉm th·∫•p
        if score < low_score_threshold:
            has_topic_match = any(kw in text_normalized for kw in relevant_keywords)
            if not has_topic_match:
                continue
        
        # Gi·ªõi h·∫°n s·ªë chunk per document
        if doc_count.get(doc_id, 0) >= max_per_doc:
            continue
            
        filtered_results.append(result)
        doc_count[doc_id] = doc_count.get(doc_id, 0) + 1
        
        # D·ª´ng khi ƒë·ªß k·∫øt qu·∫£ ch·∫•t l∆∞·ª£ng
        if len(filtered_results) >= 8:
            break
    
    return filtered_results
```

#### 2.2 MMR Diversification

```python
def mmr_diversify(results, query_embedding, lambda_param=0.7, top_k=6):
    """
    Maximal Marginal Relevance ƒë·ªÉ gi·∫£m tr√πng l·∫∑p semantic
    """
    if not results:
        return results
        
    selected = []
    remaining = results.copy()
    
    # Ch·ªçn item ƒë·∫ßu ti√™n (relevance cao nh·∫•t)
    selected.append(remaining.pop(0))
    
    while remaining and len(selected) < top_k:
        best_score = -1
        best_idx = -1
        
        for i, candidate in enumerate(remaining):
            # Relevance score
            relevance = candidate.get('relevance_score', 0)
            
            # Similarity v·ªõi c√°c ƒë√£ ch·ªçn
            max_sim = 0
            for selected_item in selected:
                similarity = compute_text_similarity(candidate['text'], selected_item['text'])
                max_sim = max(max_sim, similarity)
            
            # MMR score
            mmr_score = lambda_param * relevance - (1 - lambda_param) * max_sim
            
            if mmr_score > best_score:
                best_score = mmr_score
                best_idx = i
        
        if best_idx != -1:
            selected.append(remaining.pop(best_idx))
        else:
            break
            
    return selected

def compute_text_similarity(text1, text2):
    """T√≠nh cosine similarity ƒë∆°n gi·∫£n gi·ªØa 2 text"""
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform([text1, text2])
    return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
```

### **C·∫•p ƒê·ªô 3: C·∫£i Thi·ªán Chunking**

#### 3.1 Smart Chunking cho PDF Slides

```python
def smart_chunk_slides(text, chunk_size=500, overlap=100):
    """
    Chunking th√¥ng minh cho slide PDF - nh√≥m ti√™u ƒë·ªÅ v·ªõi n·ªôi dung
    """
    import re
    
    # Patterns cho slide headers
    slide_patterns = [
        r'^[CH∆Ø∆†NG|CHAPTER]\s+\d+',
        r'^\d+\.\d+\s+\w+',
        r'^[A-Z][^a-z]{10,}$',  # ALL CAPS headers
        r'^‚ùñ\s*\w+',  # Bullet points
    ]
    
    lines = text.split('\n')
    chunks = []
    current_chunk = ""
    current_size = 0
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Ki·ªÉm tra xem c√≥ ph·∫£i header kh√¥ng
        is_header = any(re.match(pattern, line) for pattern in slide_patterns)
        
        # N·∫øu l√† header v√† chunk hi·ªán t·∫°i ƒë√£ ƒë·ªß l·ªõn
        if is_header and current_size > chunk_size * 0.7:
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            current_chunk = line + "\n"
            current_size = len(line)
        else:
            current_chunk += line + "\n"
            current_size += len(line)
            
            # N·∫øu v∆∞·ª£t qu√° k√≠ch th∆∞·ªõc t·ªëi ƒëa
            if current_size > chunk_size + overlap:
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                # Gi·ªØ l·∫°i overlap
                overlap_text = current_chunk[-overlap:]
                current_chunk = overlap_text
                current_size = len(overlap_text)
    
    # Th√™m chunk cu·ªëi c√πng
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    return chunks
```

### **C·∫•p ƒê·ªô 4: T·ªëi ∆Øu H√≥a Database**

#### 4.1 Index T·ªëi ∆Øu

```sql
-- Vector search optimization
CREATE INDEX idx_chunks_embedding_cosine 
ON chunks USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);

-- Full-text search cho ti·∫øng Vi·ªát
CREATE INDEX idx_chunks_content_fts_vi 
ON chunks USING GIN (to_tsvector('simple', content_normalized));

-- Compound index cho filtering
CREATE INDEX idx_chunks_doc_chunk 
ON chunks (document_id, chunk_index);
```

#### 4.2 Metadata Enrichment

```sql
-- Th√™m metadata cho topic classification
ALTER TABLE documents ADD COLUMN topic VARCHAR(50);
ALTER TABLE documents ADD COLUMN language VARCHAR(10);
ALTER TABLE documents ADD COLUMN document_type VARCHAR(20);

-- Update existing data
UPDATE documents SET 
    topic = CASE 
        WHEN title LIKE '%ma%hoa%' THEN 'cryptography'
        WHEN title LIKE '%dbms%' OR title LIKE '%database%' THEN 'database'
        WHEN title LIKE '%iot%' THEN 'iot'
        ELSE 'general'
    END,
    language = CASE 
        WHEN title LIKE '%Chuong%' THEN 'vi'
        ELSE 'en'
    END;
```

## üöÄ Plan Tri·ªÉn Khai

### **Phase 1: Quick Fixes (1-2 ng√†y)**

1. ‚úÖ Th√™m `content_normalized` column v√† index
2. ‚úÖ Update keyword search v·ªõi Vietnamese normalization  
3. ‚úÖ TƒÉng `k_vec=60, k_kw=30`
4. ‚úÖ Implement `post_filter_results()`

### **Phase 2: Advanced Features (3-5 ng√†y)**

1. ‚úÖ Implement MMR diversification
2. ‚úÖ Add topic-based filtering
3. ‚úÖ Improve source citation grouping
4. ‚úÖ Add metadata enrichment

### **Phase 3: Monitoring & Tuning (ongoing)**

1. ‚úÖ Setup evaluation metrics (P@5, nDCG@10)
2. ‚úÖ A/B test different parameter combinations
3. ‚úÖ Collect user feedback for continuous improvement

## üìà Metrics ƒë·ªÉ ƒêo L∆∞·ªùng

### **Immediate Metrics**

```python
def evaluate_query_results(query, expected_docs, actual_results):
    """
    ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng k·∫øt qu·∫£ cho m·ªôt query
    """
    # Precision@K
    relevant_results = [r for r in actual_results[:5] 
                       if r['metadata']['document_id'] in expected_docs]
    precision_at_5 = len(relevant_results) / min(5, len(actual_results))
    
    # Topic coherence - t·∫•t c·∫£ results c√≥ c√πng topic kh√¥ng?
    topics = [get_document_topic(r['metadata']['document_id']) 
              for r in actual_results[:5]]
    topic_coherence = len(set(topics)) == 1
    
    return {
        'precision_at_5': precision_at_5,
        'topic_coherence': topic_coherence,
        'num_unique_docs': len(set(r['metadata']['document_id'] 
                                   for r in actual_results[:5]))
    }
```

### **Test Cases ƒë·ªÉ Validate**

```python
test_cases = [
    {
        'query': 'M√£ h√≥a kh√≥a b·∫•t ƒë·ªëi x·ª©ng l√† g√¨, c√≥ nh·ªØng thu·∫≠t to√°n n√†o',
        'expected_docs': [8],  # Ch∆∞∆°ng 3 - H·ªá m√£ b·∫•t ƒë·ªëi x·ª©ng
        'expected_keywords': ['RSA', 'ElGamal', 'DSA', 'kh√≥a c√¥ng khai']
    },
    {
        'query': 'Transaction trong SQL ho·∫°t ƒë·ªông ra sao',
        'expected_docs': [10],  # kb_dbms.pdf
        'expected_keywords': ['ACID', 'commit', 'rollback']
    },
    {
        'query': 'Client server architecture advantages',
        'expected_docs': [10, 5],  # kb_dbms + IoT book
        'expected_keywords': ['client', 'server', 'scalability']
    }
]
```

## üí° Code Examples ƒë·ªÉ Implement

### **1. Update retrieval.py**

```python
# File: api/app/retrieval.py

import unicodedata
from typing import List, Dict, Tuple
from .db import get_conn
from .llm import embed_texts

def normalize_vietnamese(text: str) -> str:
    """Chu·∫©n h√≥a ti·∫øng Vi·ªát - lo·∫°i b·ªè d·∫•u v√† lowercase"""
    return unicodedata.normalize('NFD', text.lower()).encode('ascii', 'ignore').decode('ascii')

def hybrid_search(query: str, k: int = 8, k_vec: int = 60, k_kw: int = 30) -> List[Dict]:
    """
    Improved hybrid search v·ªõi post-processing
    """
    # Vector search
    vec_hits = _vector_candidates(query, k_vec)
    
    # Keyword search v·ªõi Vietnamese normalization
    kw_hits = _keyword_candidates_normalized(query, k_kw)
    
    # Merge v√† lo·∫°i tr√πng l·∫∑p
    merged = _merge_candidates(vec_hits, kw_hits)
    
    # Rerank v·ªõi Cohere
    if len(merged) > 1:
        reranked = _rerank_with_cohere(query, merged, top_n=min(16, len(merged)))
    else:
        reranked = merged
    
    # Post-processing
    filtered = post_filter_results(reranked, query)
    final_results = mmr_diversify(filtered, query)
    
    return final_results[:k]

def _keyword_candidates_normalized(query: str, limit: int = 30) -> List[Dict]:
    """
    Keyword search v·ªõi Vietnamese normalization
    """
    normalized_query = normalize_vietnamese(query)
    
    sql_trgm = """
        SELECT id, document_id, chunk_index, content, 
               similarity(content_normalized, %s) AS score
        FROM chunks
        WHERE content_normalized ILIKE %s
        ORDER BY score DESC
        LIMIT %s
    """
    
    with get_conn() as conn:
        rows = conn.execute(sql_trgm, (
            normalized_query, 
            f'%{normalized_query}%', 
            limit
        )).fetchall()
        
    return [
        {
            "id": r[0],
            "document_id": r[1], 
            "chunk_index": r[2],
            "content": r[3],
            "score": float(r[4])
        }
        for r in rows
    ]

# ... implement post_filter_results() v√† mmr_diversify() nh∆∞ ·ªü tr√™n
```

### **2. Database Migration Script**

```sql
-- File: db/migrations/add_vietnamese_support.sql

-- B∆∞·ªõc 1: Enable unaccent extension
CREATE EXTENSION IF NOT EXISTS unaccent;

-- B∆∞·ªõc 2: Th√™m column normalized
ALTER TABLE chunks ADD COLUMN IF NOT EXISTS content_normalized TEXT 
GENERATED ALWAYS AS (unaccent(lower(content))) STORED;

-- B∆∞·ªõc 3: Th√™m indexes
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_chunks_content_norm_gin 
ON chunks USING GIN (content_normalized gin_trgm_ops);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_chunks_content_fts_vi 
ON chunks USING GIN (to_tsvector('simple', content_normalized));

-- B∆∞·ªõc 4: Th√™m metadata cho documents
ALTER TABLE documents ADD COLUMN IF NOT EXISTS topic VARCHAR(50);
ALTER TABLE documents ADD COLUMN IF NOT EXISTS language VARCHAR(10);

-- B∆∞·ªõc 5: Update existing data
UPDATE documents SET 
    topic = CASE 
        WHEN LOWER(title) LIKE '%ma%hoa%' OR LOWER(title) LIKE '%cryptograph%' THEN 'cryptography'
        WHEN LOWER(title) LIKE '%dbms%' OR LOWER(title) LIKE '%database%' THEN 'database'
        WHEN LOWER(title) LIKE '%iot%' THEN 'iot'
        ELSE 'general'
    END,
    language = CASE 
        WHEN title LIKE '%Chuong%' THEN 'vi'
        ELSE 'en'
    END
WHERE topic IS NULL OR language IS NULL;
```

## üéØ K·∫øt Qu·∫£ Mong ƒê·ª£i

Sau khi implement c√°c c·∫£i thi·ªán tr√™n:

### **Query: "M√£ h√≥a kh√≥a b·∫•t ƒë·ªëi x·ª©ng l√† g√¨, c√≥ nh·ªØng thu·∫≠t to√°n n√†o"**

```json
{
  "answer": "M√£ h√≥a kh√≥a b·∫•t ƒë·ªëi x·ª©ng hay PKC s·ª≠ d·ª•ng c·∫∑p kh√≥a c√¥ng khai-b√≠ m·∫≠t. C√°c thu·∫≠t to√°n ch√≠nh g·ªìm RSA, ElGamal, DSA, v√† ECC...",
  "sources": [
    {
      "document_id": 8,
      "title": "Ch∆∞∆°ng 3 - H·ªá m√£ b·∫•t ƒë·ªëi x·ª©ng", 
      "chunks": ["ƒê·ªãnh nghƒ©a PKC", "Thu·∫≠t to√°n RSA", "Thu·∫≠t to√°n ElGamal"]
    }
  ]
}
```

### **Improvements**

- ‚úÖ **100% relevance**: Ch·ªâ tr·∫£ v·ªÅ Ch∆∞∆°ng 3 (b·∫•t ƒë·ªëi x·ª©ng)
- ‚úÖ **ƒêa d·∫°ng n·ªôi dung**: ƒê·ªãnh nghƒ©a + c√°c thu·∫≠t to√°n c·ª• th·ªÉ  
- ‚úÖ **Ngu·ªìn s·∫°ch**: G·ªôp theo document, kh√¥ng r·∫£i r√°c
- ‚úÖ **H·ªó tr·ª£ ti·∫øng Vi·ªát t·ªët**: B·∫Øt ƒë∆∞·ª£c c√°c t·ª´ c√≥/kh√¥ng d·∫•u

---

**üìû Contact & Support**: S·∫µn s√†ng h·ªó tr·ª£ implement t·ª´ng phase v√† debug khi c·∫ßn thi·∫øt!
